{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. ML Monitoring metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1. How to evaluate ML model quality"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Challenges of standard ML monitoring\n",
    "Standard ML monitoring means measuring the ML model performance such as:\n",
    "    - **Model quality and error metrics:** It assesses the model performance in production such as accuracy, precision, recall, F1 score, etc.\n",
    "    - **Buisness or product metrics:** It assesses the business impact of the model in production such as purchases, clicks, visits, etc.\n",
    "\n",
    "- **Challenges**\n",
    "    - *delayed ground truth:* The ground truth is not available immediately after the model prediction. This makes calculating model performance metrics impossible.\n",
    "    - *Past performance does not guarantee future performance:* This is because the data distribution in the future may change.\n",
    "    - *Segments with different quality* \n",
    "    - *Volatile target function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Early monitoring metrics\n",
    "**Early monitoring metrics** are metrics that can be calculated before the ground truth is available. It is used to track the issues with:\n",
    "- **Data Quality**\n",
    "- **Data drift**\n",
    "- **Output drift:** It occurs when the model's predictions become less accurate because the true values in the dataset have shifted or evolved."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2. Evaluating ML model quality in production"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification quality metrics\n",
    "- **Accuracy:** The proportion of correctly classified instances out of the total instances. This metric fails with imbalanced data.\n",
    "\n",
    "        $Accuracy = \\frac{TP + TN}{TP + TN + FP + FN}$\n",
    "- **Precision:** The proportion of true positive predictions out of all positive predictions.\n",
    "\n",
    "        $Precision = \\frac{TP}{TP + FP}$\n",
    "- **Recall:** The proportion of true positive predictions out of all actual positive instances.\n",
    "    \n",
    "        $Recall = \\frac{TP}{TP + FN}$\n",
    "- **F1 score:** The harmonic mean of precision and recall, providing a balance between the two metrics.\n",
    "\n",
    "        $F1 = \\frac{2}{\\frac{1}{Precision} + \\frac{1}{Recall}}$\n",
    "- **AUC-ROC:** The area under the Receiver Operating Characteristic (ROC) curve, which measures the model's ability to distinguish between positive and negative classes.\n",
    "- **Log loss:** It measures how close is the prediction from the actual value. \n",
    "\n",
    "        $LogLoss = -\\frac{1}{N}\\sum_{i=1}^{N}(y_i\\log(p_i) + (1-y_i)\\log(1-p_i))$\n",
    "- **Confusion matrix:** A table that shows the number of correct and incorrect predictions made by the model compared to the actual outcomes which are TP, FP, TN, FN.\n",
    "- **Precision-recall table:** A table that shows the precision and recall values for different thresholds.\n",
    "- **Class separation quality:** It helps to understand how well the model separates the classes. It is measured by the distance between the distributions of the model's predictions for each class.\n",
    "- **Error analysis:** It helps to understand the model's errors and the reasons behind them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regression quality metrics\n",
    "- **Mean Absolute Error (MAE):** The average of the absolute differences between predictions and actual values.\n",
    "\n",
    "        $MAE = \\frac{1}{N}\\sum_{i=1}^{N}|y_i - \\hat{y_i}|$\n",
    "- **Mean Squared Error (MSE):** The average of the squared differences between predictions and actual values.\n",
    "    \n",
    "        $MSE = \\frac{1}{N}\\sum_{i=1}^{N}(y_i - \\hat{y_i})^2$\n",
    "- **Root Mean Squared Error (RMSE):** The square root of the average of the squared differences between predictions and actual values.\n",
    "\n",
    "        $RMSE = \\sqrt{\\frac{1}{N}\\sum_{i=1}^{N}(y_i - \\hat{y_i})^2}$\n",
    "- **Predicted VS Actual Values:** A plot that shows the predicted values against the actual values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ranking quality metrics\n",
    "Ranking focuses on the relative order rather then their absolute values like Search engines and recommending systems. This are the metrics used\n",
    "- **Cumulative Gain** \n",
    "- **Discounted Cumulative Gain (DCG)**\n",
    "- **Normalized DCG (NDCG)**\n",
    "- **Precision@K**\n",
    "- **Recall@K**\n",
    "-  **Lift@k**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3. Data quality in Machine Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What can go wrong with the input data?\n",
    "- **Wrong source**\n",
    "- **Lost access**\n",
    "- **Bad SQL or not SQL**\n",
    "- **Infrastructure update**\n",
    "- **Broken feature code**\n",
    "- **Data loss or corruption**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data quality metrics and analysis\n",
    "- **Data profiling:** it involves the basic descriptive stats for the dataset such us Min Max values, Quantiles, unique values, most common values, etc. It also includes also data visualization and data distribution analysis.\n",
    "- **Manual Thresholds:** In the absence of reference data, establish data quality criteria manually based on domain knowledge.\n",
    "    - Minimal missing values.\n",
    "    - No duplicate rows or columns.\n",
    "    - Avoid constant or highly correlated features.\n",
    "    - Prevent target leaks (high feature-target correlations).\n",
    "    - Enforce logical ranges, considering feature context (e.g., age cannot be negative).\n",
    "- **Reference Data:** Having reference data simplifies the process and enables automated checks.\n",
    "\n",
    "    - Compare current data to the reference dataset.\n",
    "    - Automatically assess data schema, completeness, batch size, and specific column patterns.\n",
    "    - Check for unique/non-unique features and expected data distributions.\n",
    "    - Set criteria based on observed value ranges and descriptive statistics like averages and medians."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4. Data and prediction drift in ML"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is data drift and why evaluate it?\n",
    "In case of the ground truth data unvailability, proxy metrics are used such as feature and prediction drift.\n",
    "- **Prediction drift:** It tracks changes in the distribution of the model predictions over time. If a change is detected it can be an early sign of changes in the model environment.\n",
    "- **Feature drift:** It tracks changes in the distribution of the input features. It can be an early warnnig of the model quality degradation, data quality issues, or changes in the model environment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How to detect data drift\n",
    "- **Drift detection method:** such as statistical tests, distance metrics, etc.\n",
    "- **Drift detection threshold** \n",
    "- **Reference dataset**\n",
    "- **Alert conditions**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Univariate vs. multivariate drift\n",
    "- **Univariate drift:** It tracks the drifts in the distribution of each feature independently. It's easy to understand because we're looking at one feature at a time.\n",
    "- **Multivariate drift:** It tracks the drifts in the distribution of the hole dataset. It's useful when dealing with many features or complec interactions.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tips for calculating drift\n",
    "- **Calculate data quality**\n",
    "- **Mind the feature set:** The approach to drift analysis varies based on the type and importance of features.\n",
    "- **Mind the segment based-drift** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Some key consideration about data drift\n",
    "- **Prediction Drift Matters Most:** Changes in what the model predicts are often more critical than changes in the input data.\n",
    "- **Data Drift is Flexible:** What's considered \"drift\" depends on the specific situation and data you're dealing with.\n",
    "- **Drift Doesn't Always Harm Models:** Consider the context, importance of features, and use case.\n",
    "- **Data Drift Monitoring is Selective:** It's especially useful for important models with delayed feedback, but it's not always necessary.\n",
    "- **Data Drift Helps Debugging:** tracking it can help troubleshoot issues in the model.\n",
    "- **Drift Detection is Valuable with Labels:** Changes in features might show up before the model's performance drops."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
